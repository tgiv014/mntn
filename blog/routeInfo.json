{"template":"__react_static_root__/src/pages/blog.js","sharedHashesByProp":{},"data":{"posts":[{"fname":"project-euler-15-addendum.md","data":{"title":"Project Euler Problem #15 - Addendum","date":"2021-02-06 00:00:00 -0500","categories":"code"},"md":"<p><em>This is an addendum to <a href=\"/blog/post/project-euler-15\">this previous post</a>.</em></p>\n<h1>It's learnin' time</h1>\n<p>While completing advent of code 2020, I learned about <a href=\"https://en.wikipedia.org/wiki/Memoization\">memoization</a>. I realized that this is a perfect problem to apply the technique to. All we need is a hashmap indexed by our tree coordinates that we'll pass in our recursive function. If there's a cached value for the function inputs, we early return the cached value. In our normal return path we save off the calculated value before actually returning it. It's a simple formula for faster processing.</p>\n<p>Without further ado:</p>\n<pre><code class=\"language-rust\">use std::collections::HashMap;\n\n#[derive(Hash, Eq, PartialEq, Debug)]\nstruct Coordinate {\n    x: u32,\n    y: u32,\n}\n\n// The size of our domain in steps\n// The grid is NxN steps\n// Really, our position grid is N+1xN+1\n// [0,0] describes the start and [N,N] describes the end point\nstatic N:u32 = 20;\n\nfn build_node(x:u32, y:u32, cache:&#x26;mut HashMap&#x3C;Coordinate, u64>) -> u64 {\n    let c = Coordinate{x,y};\n    let mut sum:u64 = 0;\n    if cache.contains_key(&#x26;c) {\n        return cache[&#x26;c];\n    }\n    // If x==y==N, we're at the end point!\n    if x == N &#x26;&#x26; y == N {\n        return 1;\n    }\n    // Recurse for each child node if possible\n    if x &#x3C; N {\n        sum += build_node(x + 1, y, cache);\n    }\n    if y &#x3C; N {\n        sum += build_node(x, y + 1, cache);\n    }\n    \n    cache.insert(c, sum);\n    return sum;\n}\n\nfn main() {\n    let mut cache:HashMap&#x3C;Coordinate, u64> = HashMap::new();\n    let n_ends:u64 = build_node(0, 0, &#x26;mut cache);\n    println!(\"Total unique paths: {}\", n_ends);\n}\n</code></pre>\n<p>Now it takes ~5ms to evaluate every path in a 20x20 grid! That's a heck of a lot better than 30min.</p>\n","stripped":"project-euler-15-addendum"},{"fname":"requests-cache.md","data":{"title":"Faster Backtesting with requests-cache","date":"2021-02-03 00:00:00 -0500","categories":"code"},"md":"<h1>The Problem</h1>\n<p>Lately, I've been playing with algorithmic trading using <a href=\"https://www.backtrader.com/\">Backtrader</a>, which is a spectacular tool for experimenting with algorithmic trading strategies. This hobby could be considered a problem in itself, but that's not today's topic. A strategy I've been testing involves maintaining a list of S&#x26;P500 stocks, ranking them by some metrics, and making a portfolio out of the top 20% or so. Essentially, a really impersonal buy-and-hold strategy.</p>\n<p>Unfortunately, this means I need to collect data on 500 stocks over my backtesting time range (5+ years). I would really prefer to use Backtrader's automatic Yahoo Finance data source <code>bt.feeds.YahooFinanceData</code>, but I don't want to send 500+ requests to Yahoo every time I tweak a single variable. On a list of just 16 stocks, it takes ~40 seconds to query 5 years of data and run the strategy. There has to be a better way.</p>\n<h1>Enter <code>requests-cache</code>!</h1>\n<p>Backtrader supports proxies (so we could set up a caching proxy), but under the hood, backtrader's <a href=\"https://github.com/mementum/backtrader/blob/master/backtrader/feeds/yahoo.py\">Yahoo feed</a> uses the <code>requests</code> module! This means we can solve our problem without even touching a proxy service or having to set up local SSL certificates to cache HTTPS.</p>\n<p>All we need to do is run <code>pip install requests-cache</code> and add two lines of code to the start of our Backtrader script.</p>\n<pre><code class=\"language-python\">import requests_cache\nrequests_cache.install_cache('test_cache', expire_after=3600)\n</code></pre>\n<p>Now, requests-cache will automatically cache any requests made via the python <code>request</code> module in a file called <code>test_cache.sqlite</code>. Even HTTPS. With this little change, backtesting against 16 stocks over 5 years finishes in 10 seconds: an improvement of 75%!</p>\n","stripped":"requests-cache"},{"fname":"project-euler-15.md","data":{"title":"Project Euler Problem #15","date":"2020-11-13 17:00:00 -0500","categories":"code"},"md":"<p><em>Doing the right things the wrong way.</em></p>\n<h1>Intro</h1>\n<p><a href=\"https://projecteuler.net/problem=15\">Problem #15</a> from <a href=\"https://projecteuler.net/about\">Project Euler</a> is a really interesting problem. Not only is there a clean mathematical solution to the question, but there are a <em>bunch</em> of non-optimal (or even downright ugly) but intuitive ways to solve the problem.</p>\n<h1>The Easy Way</h1>\n<p>The easiest, and probably fastest, way to solve this problem is to just use math. Consider the rules of the problem:</p>\n<ol>\n<li>Each path is composed of <em>2N</em> steps that lead from the top left to the bottom right of the domain.</li>\n<li>Each step can only be a rightward or downward move.</li>\n<li>Because of the start and end positions, each path must have an equal number of down and right moves.</li>\n</ol>\n<p>Based on these rules, you can build a valid path by starting with an empty list of <em>2N</em> steps, selecting <em>N</em> steps to place a rightward move, and filling the remaining steps with downward moves. This is a <a href=\"https://en.wikipedia.org/wiki/Combination\">Combination Problem</a>! The number of valid paths you can possibly construct is <em>2N choose N</em>.</p>\n<h1>A Fun Way</h1>\n<center>\n<img src=\"/img/treediagram.png\"/>\n</center>\nMaybe you're itching for a reason to think about binary trees. You can look at every step in a path as a node in a binary tree. The starting position is the root node. The root node has two child nodes, one for a downward move and one for a rightward move. Each of those nodes have their own children in the same pattern. This means that two of the \"grandchild\" nodes actually describe different paths to the same point. In the above picture, downward moves are to the left and rightward to the right.\n<p>Using this tree, we can describe any path on an infinite grid. To constrain the grid (and know when we've reached the end point), we need to add some state to each node: an x and y position: (x,y). To create a new downward node in our domain, y must be less than <em>N</em> - the new node's y value will be increased by 1. To create a new rightward node, x must be less than <em>N</em> - we'll increase the node's x. If a new node has state <em>x == N</em> and <em>y == N</em>, we've hit the end point of the grid!</p>\n<p>If we actually build out this tree to full <em>2N</em> depth, we'll eat up a ton of memory. Conveniently, we don't actually need to hold all of this state since we're only interested in how many nodes in the tree are at the end point. We can just pretend to build the tree with a recursive function. Here's an implementation in RustðŸ¦€:</p>\n<pre><code class=\"language-rust\">// The size of our domain in steps\n// The grid is NxN steps\n// Really, our position grid is N+1xN+1\n// [0,0] describes the start and [N,N] describes the end point\nstatic N:u32 = 20;\n\nfn build_node(x:u32, y:u32, n_ends:&#x26;mut u64) {\n    // If x==y==N, we're at the end point!\n    if x == N &#x26;&#x26; y == N {\n        *n_ends += 1;\n        return;\n    }\n    // Recurse for each child node if possible\n    if x &#x3C; N {\n        build_node(x + 1, y, n_ends);\n    }\n    if y &#x3C; N {\n        build_node(x, y + 1, n_ends);\n    }\n}\n\nfn main() {\n    let mut n_ends:u64 = 0;\n    // This describes the root node\n    build_node(0, 0, &#x26;mut n_ends);\n    println!(\"Total unique paths: {}\", n_ends);\n}\n</code></pre>\n<p>Go get a coffee or two, because this took ~30min to run on my machine.</p>\n<p><em>Edit: This can be heavily optimized without abandoning the tree structure! More info in <a href=\"/blog/post/project-euler-15-addendum\">this more recent post</a>.</em></p>\n<h1>An ugly way</h1>\n<p>Suppose you were told to solve this problem on an FPGA and you have <em>no clue</em> what combination is. Since we know the length of a path is <em>2N</em>, we'll start with a <em>2N</em>-bit wide counter. Let's say down = 1 and right = 0. We can initialize a down-counter with all 1s and let it run all the way down to 0. Since we know a valid path has #rights=#downs=<em>N</em>, we'll look at every counter value and keep track of the number of values which have exactly <em>N</em> ones. That number is the number of possible paths!</p>\n<p>We can even slightly optimize this by recognizing that we only have to evalute all of the paths that start with a downward move and multiply the result by two. This follows from our tree representation above. Rust example below:</p>\n<pre><code class=\"language-rust\">// The size of our domain in steps\nstatic N:u32 = 20;\n\nfn main() {\n    let n_ends:u64 = (2u64.pow(2*N-1)..2u64.pow(2*N)).map(|x| (x.count_ones()==N) as u64).sum();\n    println!(\"Total unique paths: {}\", 2*n_ends);\n}\n</code></pre>\n<p>Go get another coffee. This took ~15 minutes on my machine. In the end, the mathematical solution is the way to go, but it's still fun to explore other solutions.</p>\n","stripped":"project-euler-15"},{"fname":"docker-zfs-deb-1.md","data":{"title":"Docker & ZFS On Debian","date":"2020-10-03 9:45:00 -0500","categories":"homelab"},"md":"<p><em>Building a capable Docker Machine on a Dell R710</em></p>\n<center>\n<img src=\"/img/zfs_docker_debian.png\"/>\n</center>\n<h1>How'd we Get Here...</h1>\n<p>A while back, I snagged a Dell R710 from Ebay for $350 delivered. This thing is a virtualization <em>monster</em>. 8 cores and 144GB of ECC RAM, perfect for filling with bhyve virtual machines on FreeNAS. This setup worked great for me until I became interested in Docker containers. FreeNAS just doesn't support a docker environment, so let's build something from the ground up and document the process.</p>\n<h1>Constraints</h1>\n<ul>\n<li>Must use ZFS (no need for ZFS root)</li>\n<li>Native docker support</li>\n<li>Lightweight (no GUI, unnecessary package systems *cough cough snaps*, etc.)</li>\n</ul>\n<p>With these constraints, we know we absolutely must use Linux and we would really like a distro that has openzfs in its package control. In the past I've used Ubuntu Server and ubuntu 16.04 has zfs available as a package, but I want to avoid snaps like the plague. Because of this, I'm going to use Debian.</p>\n<h1>Initial Setup</h1>\n<p>First things first, we're going to shut down the server and remove every drive but our desired boot drive. FreeNAS uses a flash drive as its root storage, but debian doesn't quite support that. In my case, I'm going to use an old 120GB SSD in an optical drive caddy in place of the server's disk drive. It's easy enough to complete the guided install, and Debian has some pretty good installation literature:</p>\n<ul>\n<li><a href=\"https://www.debian.org/releases/stable/i386/index.en.html\">Installation Guide - Long</a></li>\n<li><a href=\"https://www.debian.org/releases/stable/i386/apa.en.html\">Installation Howto - Short</a></li>\n</ul>\n<p>I do have a few gotchas during the guided install:</p>\n<ul>\n<li>Some NICs require closed-source firmware to operate. The NIC in my R710 is one of those, so I had to use the <a href=\"https://cdimage.debian.org/cdimage/unofficial/non-free/cd-including-firmware/current/\">non-free debian image</a>.</li>\n<li>Guided partitioning failed for my install. I suspect that this is because it was trying to make a gigantic swap partition to match the 144GB of RAM. I decided not to use a swap partition and partition the entire SSD as <code>/</code>.</li>\n<li>I avoided giving root a password. Instead, I have an administrator user with sudo access.</li>\n<li>I did not install any desktop environments or X server components.</li>\n<li>Don't forget to select openssh!</li>\n</ul>\n<h1>Post-Install</h1>\n<p>Now that we have a fully running debian system, let's change a few things. Pop all your drives back in and ssh in.</p>\n<p>Let's gear up to use ssh without a password, starting by setting up private key authentication. If you're on linux, you've got it easy: <code>$ ssh-copy-id $USER@$HOST</code>. On Windows, I find it easiest to manually copy the contents of <code>%HOMEPATH%\\.ssh\\id_rsa.pub</code> to <code>~/.ssh/authorized_keys</code> on my server. If you don't have an <code>~/.ssh/id_rsa.pub</code> or <code>%HOMEPATH%\\.ssh\\id_rsa.pub</code>, you better generate an ssh key with <code>$ ssh-keygen</code>.</p>\n<p>Once your public key is copied in, it's a good idea to exit your ssh session and attempt to login with private key auth. If you don't need a password, you're on the right path. Time to disable password authentication. In <code>/etc/ssh/sshd_config</code>, find the lines containing the following keys, uncomment them and make sure they're set to <code>no</code>.</p>\n<pre><code>PasswordAuthentication no\nChallengeResponseAuthentication no\nPermitRootLogin no\nUsePAM no\n</code></pre>\n<p>Now you can reload sshd and bask in paswordless security strong enough to leave exposed publicly. <code>$ sudo /etc/init.d/sshd reload</code></p>\n<h1>ZFS Time</h1>\n<p>Debian makes this one super easy. Check the <a href=\"https://wiki.debian.org/ZFS\">official ZFS Debian Wiki Page</a> for more info. Use the following commands to install ZFS on Debian.</p>\n<pre><code class=\"language-bash\">$ sudo apt update\n$ sudo apt install linux-headers-`uname -r`\n$ sudo apt install -t buster-backports dkms sol-dkms\n$ sudo apt install -t buster-backports zfs-dkms zfsutils-linux\n</code></pre>\n<p>It may take a little while to run the dkms build, and it may throw a few warnings. They do not specifically recommend a reboot on the Debian Wiki, but it's never a bad idea when dealing with kernel modules.</p>\n<p>Just for grins, if you happen to be installing on a system that used to run FreeNAS, you can try to import your old pool with <code>$ sudo zpool import</code>. You're likely to see <code>action: The pool can be imported using its name or numeric identifier and the '-f' flag.</code>. You can force the import by running <code>$ sudo zpool import -f $POOLNAME</code>.</p>\n<p>If you're starting fresh, now is the time to make your first zfs pool. I'm in a bit of a hard drive shortage, so for testing's sake I will be doing a stripe pool with two 500GB hard drives. ZFS uses disk IDs or paths to reference disks instead of /dev/sdX paths. You can figure out which disk is which with the following commands:</p>\n<pre><code>$ ls -l /dev/disk/by-id\n$ ls -l /dev/disk/by-path\n$ lsblk -o NAME,SIZE,MODEL,VENDOR\n</code></pre>\n<p>With the device IDs in hand, I'll make a zpool with:</p>\n<pre><code>$ sudo zpool create tank ata-ST9500325AS_6VESPM0A ata-ST9500325AS_6VESRNXY\n</code></pre>\n<h1>Docker Install</h1>\n<p>This is essentially a summary of the <a href=\"https://docs.docker.com/engine/install/debian/\">Docker Docs for Debian</a>.</p>\n<ol>\n<li>Update apt and install required packages:</li>\n</ol>\n<pre><code>$ sudo apt update\n$ sudo apt install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n</code></pre>\n<ol start=\"2\">\n<li>Get Docker's GPG key:</li>\n</ol>\n<pre><code>$ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -\n</code></pre>\n<p>You can verify you have the correct key as follows:</p>\n<pre><code>$ sudo apt-key fingerprint 0EBFCD88\npub   4096R/0EBFCD88 2017-02-22\n      Key fingerprint = 9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88\nuid                  Docker Release (CE deb) &#x3C;docker@docker.com>\nsub   4096R/F273FCD8 2017-02-22\n</code></pre>\n<ol start=\"3\">\n<li>Use the following command to set up Docker stable.</li>\n</ol>\n<pre><code>$ sudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/debian \\\n   $(lsb_release -cs) \\\n   stable\"\n</code></pre>\n<ol start=\"4\">\n<li>Finally, install docker!</li>\n</ol>\n<pre><code>$ sudo apt update\n$ sudo apt install docker-ce docker-ce-cli containerd-io\n</code></pre>\n<ol start=\"5\">\n<li>If you want to allow other users to run docker commands, you can set that up <a href=\"https://docs.docker.com/engine/install/linux-postinstall/\">as follows</a>:</li>\n</ol>\n<pre><code>$ sudo groupadd docker\n$ sudo usermod -aG docker $USER\n</code></pre>\n<p>Log out and back in</p>\n<ol>\n<li>You can verify that your docker install is functional by running <code>hello-world</code></li>\n</ol>\n<pre><code>$ docker run --rm hello-world\n</code></pre>\n<h1>Let's Hook up Docker and ZFS</h1>\n<p>This part's for the adventurous. You could certainly just make a bunch of ZFS datasets with filesystem mounts and mount those in your containers for bulk storage, but that's no fun! Docker has a <a href=\"https://docs.docker.com/storage/storagedriver/zfs-driver/\">ZFS storage driver</a> that will back all container, image, and volume storage with zfs. Let's set it up.</p>\n<ol>\n<li>Kill docker:</li>\n</ol>\n<pre><code>$ sudo /etc/init.d/docker stop\n</code></pre>\n<ol start=\"2\">\n<li>Back up /var/lib/docker just in case and then delete its contents:</li>\n</ol>\n<pre><code>$ sudo cp -au /var/lib/docker /var/lib/docker.bk\n$ sudo rm -rf /var/lib/docker/*\n</code></pre>\n<ol start=\"3\">\n<li>Create a ZFS-backed mountpoint at <code>/var/lib/docker</code>. The official docker instructions suggest making a zpool and mounting it there, but I'll use a dataset since I already have a zpool configured (name <code>tank</code>).</li>\n</ol>\n<pre><code>$ sudo zfs create -o mountpoint=/var/lib/docker tank/docker\n</code></pre>\n<ol start=\"4\">\n<li>Tell Docker to use ZFS for its storage driver. In <code>/etc/docker/daemon.json</code>:</li>\n</ol>\n<pre><code>{\n  \"storage-driver\": \"zfs\"\n}\n</code></pre>\n<ol start=\"5\">\n<li>Start docker back up and verify you're using the ZFS storage driver:</li>\n</ol>\n<pre><code>$ sudo /etc/init.d/docker start\n$ docker info\nServer:\n Containers: 0\n  Running: 0\n  Paused: 0\n  Stopped: 0\n Images: 114\n Server Version: 19.03.11\n Storage Driver: zfs\n  Zpool: tank\n  Zpool Health: ONLINE\n  Parent Dataset: tank/docker\n  Space Used By Parent: 17283463680\n  Space Available: 935802788864\n  Parent Quota: no\n  Compression: off\n Logging Driver: json-file\n Cgroup Driver: cgroupfs\n</code></pre>\n<h1>ðŸŽ‰ Done!</h1>\n<p>Now we have a server running Debian with Docker using ZFS-backed storage. This would be a great way to put together a NAS setup using samba or NFS with a bunch of self-hosted services. I'll be using Docker to host a home assistant instance with nginx sitting in front as a reverse proxy.</p>\n","stripped":"docker-zfs-deb-1"}]},"path":"blog"}
